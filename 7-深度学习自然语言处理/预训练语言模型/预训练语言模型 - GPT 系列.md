# 预训练语言模型 - GPT 系列

## GPT 1.0 [1]

GPT 1.0 与 Bert 很相似，同样是作为二阶段的模型，只是在细节处可能相差较大。



**1， 语言模型**

对于给定的 tokens $U = \{u_1, \cdots, u_n \}$ ， GPT 1.0 的语言模型的目标函数如下：
$$
L_1(U) = \sum_i log P(u_i| u_{i-k}, \cdots, u_{i-1}; \Theta); \, \, \text{k为窗口大小}
$$
从上式可以看出， GPT 1.0 是具有句子的生成能力的， 可惜 GPT 1.0 没有关于生成方面的实验。

**2， 单向Transformer**

GPT 1.0 采用单向的 Transformer 来作为特征抓取器，这是由于语言模型本身决定的，因为是从前往后生成单词的。

**3， 微调**

![](http://ww1.sinaimg.cn/large/006gOeiSly1fwc9ch5yd3j30ua0h7gp9.jpg)

微调阶段也没啥好说的，也就那样，这里就不细细讨论了。

## GPT 2.0

**1， 语言模型与单向Transofmer**

GPT 2.0 的语言模型与 GPT 1.0 很相似， 变化很少，如Layer Normalization 的转移和添加等，且这部分也不是文章的主要创新点，因此我略过了。

**2， 大就完事了**

GPT 2.0 相较 1.0 来说，在数据方面的改动很大，主要包括以下几个方面： 

- 大规模，高质量，跨领域数据：WebText
- 更深的 Transoformer 模型

GPT 2.0 验证了数据的重要性，即使单纯的从数据角度入手，效果就可以获得巨大的提升。

**3， 如何适配下游任务**

对于下游任务来说， GPT 2.0 不再采用微调的方式来做，而是直接用训好的语言模型来做，那么它怎么就能够又能做分类，又能做翻译，还能做文本摘要的呢？

答案很风骚： GPT 2.0 在做下游任务时，添加了一些引导字符来预测目标，它的输出与语言模型一样，都是一个单词。 

那么 GPT 2.0 是如何做生成类任务的呢？ 那就是连续不断的进行预测，预测 n 次(设定)， 然后把这 n 个token 连在一起，取其中的几句话来做为生成的文本。

**4， 思想的转变**

GPT 2.0 相较GPT 1.0 而言，改变最大的思想，具体来说， GPT 2.0 依旧是二阶段的框架，但对于下游任务，不再采用有监督的进行微调，而是采用无监督的方式直接去做。

作者认为，通过大模型，大规模数据，GPT 2.0 能够学习到很多通用知识，直接拿这些通用知识去做下游任务就可以获得很好的结果。 这其实就是**证明预训练语言模型这条道路的正确性，预训练语言模型的确能够学习到语言的很多信息，并具有很强的泛化能力。** 

但， 真的不需要微调吗？ 我认为接下来一段时间的预训练语言模型的发展依旧会是二阶段或三阶段的框架： **预训练语言模型 + [多任务学习] + [在特定数据集上预训练] + 下游任务微调。**

不可否认的是， GPT 2.0 同样打开了一个新思路， **如果有一天， 模型足够大，数据足够多，我们还需要微调吗？**



## GPT 3.0





## Questions







## Reference

[1] GPT 1.0: Improving Language Understanding by Generative Pre-Training

[2] GPT 2.0: Language Models are Unsupervised Multitask Learners

[3] GPT 3.0: Language Models are Few-Shot Learners

