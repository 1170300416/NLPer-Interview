# 如何更好的预训练一个BERT







## Reference

Train No Evil- Selective Masking for Task-Guided Pre-Training

Don’t Stop Pretraining- Adapt Language Models to Domains and Task

[Recent Advances in Language Model Fine-tuning](https://ruder.io/recent-advances-lm-fine-tuning/)

