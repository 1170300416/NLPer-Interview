# 词向量与语言模型

词向量，语言模型，预训练语言模型：NNLM， word2vec， （Glove， Fasttext） ，预训练语言模型深入：ELMO，GPT，GPT1，GPT2，ERINE，ERINE2.0，albert，Robert， xlnet 等变种。



## 1. 语言模型基础与词向量

语言模型可以简单理解为一个句子 s 在所有句子中出现的概率分布 P(s)。比如一个语料库中有100 个句子，『OK』这个句子出现了5次， 那么 $p(OK) = 5\%$。

那么，如何学习到这种概率分布呢？ 最简单的方法是建立一个无比庞大的语料库，该语料库中包含了人类成百上千年间可能讲过的所有的话，那么我们不就可以算出这句话出现的概率了吗。可惜此方法不现实。 

那么，能不能通过数学的方法进行表示呢？答案是可以滴，因为 S 是一个序列 $w_1, w_2 , \cdots ,w_n$，那么$p(s)$可以展开为：
$$
p(s) = P(w_1, w_2, \cdots, w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1,w_2) \cdots P(w_n|w_1, w_2, \cdots, w_{n-1})
$$
那么现在的问题就变成了我们如何计算 $P(w_1, w_2, \cdots, w_n) $

### 统计方法 - n元模型

我们观察上式，会发现， $P(w_1)$ 比较好算， $P(w_2|w_1)$  也还ok， $P(w_3|w_1,w_2)$ 就比较有难度了，随着n的增大，计算会越来越难， $P(w_n|w_1, w_2, \cdots, w_{n-1})$ 几乎根本不可能估算出来。怎么办？

马尔可夫假设：假设任意一个词 $w_i$  出现的概率只同它前面的n个词 $(w_{i-1}, \cdots, w_{i-n})$ 有关。由此，那么就有：
$$
一元模型：  P(S) = \prod_{i=1}^{l} P(w_i)   \\
二元模型：P(S) = \prod_{i=1}^{l} P(w_i|w_{i-1}) 
$$
缺陷： 

1. 无法建模更远的关系，语料的不足使得无法训练更高阶的语言模型。
2. 无法建模出词之间的相似度。
3. 训练语料里面有些 n 元组没有出现过,其对应的条件概率就是 0,导致计算一整句话的概率为 0。解决这个问题有两种常用方法： 平滑法和回退法。

### 深度学习方法 - 神经网络语言模型[1]

首先，我们回到问题本身，我们为什么要计算 $p(s)$， 我们的目的是为了通过大规模语料库学习到语言内部的概率分布。那么有没有办法通过深度学习的方式来学习到这种概率分布呢？

![](https://pic3.zhimg.com/v2-a58e75cd7fcca491713d8601924d3d46_b.png)

 观察上图，假设有一组词序列： $w_1, w_2, \cdots, w_t$ ，其中 $w_i \in V$ ， $V$ 是所有单词的集合。我们的输入是一个词序列，而我们的输出是一个概率值，表示根据context预测出下一个词是 $i$ 的概率。用数学来表示，我们最终是要训练一个模型： 
$$
P(w_t = i | context) = P(w_t | w_1^{t-1}) 
$$

- $w_t $表示这个词序列中的第 $t$ 个单词， $w_{t-n+1}$ 表示输入长度为n的词序列中的第一个单词
- $w_1^{t-1}$ 表示从第1个单词到第 $t-1$ 个单词组成的子序列

因此我们发现，该模型的每个样本其实计算的是：$p(w_n|w_1, \cdots, w_{n-1})$

### 词向量 - 表示语言的方式

前面我们通过 NNLM 可以知道，通过语言模型的训练，模型可以学习到语言的概率分布，那么如何将学习到的信息应用到下游任务呢？ 这就是词向量产生的背景，如何用向量来表示语言信息，这里我们简单介绍下 Word2vec[4]。

首先明确一点的是， **词向量是语言模型的副产物**。 怎么理解呢，意思是说，词向量是语言模型训练完成后产生的。

这里我们以word2vec 的 CBOW 训练模型为例：

![img](https://pic4.zhimg.com/v2-27f3e577618f84c0026968d273d823ef_b.jpg)
$$
h = \frac{1}{C} W^T(x_1 + x_2 + \cdots + x_c) \\
u = W'^{T} * h  \\
P(w_j| context)  =y_i =  \frac{exp({u_j})}{\sum_{k \in V} exp({u_k})}
$$
通过这样训练完成后，把 $W_{V\times N}$ 保存下来，我们就得到了词向量。

## 基础神经单元





## 2. 预训练语言模型 - 用模型表示语言

前面提到，我们的最终目的还是通过语言模型来获得某种语言的表示，但是我们看到，上面那种训练方式，似乎不太合适，那个最后$W_{N\times V}$ 消失不见了，并且模型无法做深。我们此处总结一下word2vec的弱点：

- 模型无法做深，词向量的表征能力有限，词向量的抽象程度不高
- 词向量获得的是上下文无关的，难以解决歧义问题上
- OV 词无法解决

很有意思的是， 相差几个月的时间， ELMO ，GPT， BERT 相继诞生了，都非常具有代表性。 下面会进行分别介绍。

### 1. ELMO 

![](https://pic4.zhimg.com/v2-78ee0e1d4b79b5e190f4241e2f0d9577_r.jpg)

elmo 是通过 L 层的双向LSTM语言模型来学习上下文信息的，这就解决了上文提到的前两个问题，而针对 OV词， ELMO 采用了 char-level 来生成词向量进而进行训练。而对于ELMO 的不同层而言，不同层的 LSTM 能够把握不同粒度和层级的信息，比如浅层的 LSTM 把握的是单词特征， 中层的 LSTM 把握 句法 特征， 深层的 LSTM 把握语义特征。
$$
前向语言模型：p(t_1, t_2, \cdots, t_N) = \prod_{k=1}^N p(t_k|t_1, t_2, \cdots, t_{k-1})   \\
后向语言模型：p(t_1, t_2, \cdots, t_N) = \prod_{k=1}^N p(t_k|t_{k+1}, t_{k+2}, \cdots, t_{N}) 
$$
但， ELMO 的缺点也十分明显：

- **LSTM 特征抽取能力远弱于 Transformer ， 并行性差**
- **拼接方式双向融合特征融合能力偏弱**

### 2. BERT



### 3. GPT系列

GPT1.0 采用**单向 Transformer** 作为特征抽取器

### 





## 3. 如何使用预训练语言模型

### 1. 是否要进行微调[1]

我们是直接采用训练好的向量还是用预训练语言模型进行微调呢？实际上，对于大多数的任务， BERT 进行微调的方式总是比提取向量再训练的方式能够获得更佳的效果。因此，在条件允许的情况下，推荐采用微调的方式。

### 2. 是否要进行再次预训练[2]

答案是需要， 我们知道现在的预训练语料采用的都是百科，书籍等比较规范的数据，而实际业务中的数据千差万别，可以这么理解，预训练本身获得的是语料库中文本的分布，而如果预训练数据分布与业务数据分布偏差较大，会带来一些负面影响。 

因此，针对一些业务，如果数据与百科数据差别非常大，先进行预训练，然后再进行微调是一种比较合适的方式。

我们这里简单介绍下[2] 中的结论

1. 在目标领域的数据集上继续预训练（DAPT）可以提升效果；目标领域与语言模型的原始预训练语料越不相关，DAPT效果则提升更明显。
2. 在具体任务的数据集上继续预训练（TAPT）可以十分“廉价”地提升效果。
3. 结合二者（先进行DAPT，再进行TAPT）可以进一步提升效果。
4. 如果能获取更多的、任务相关的无标注数据继续预训练（Curated-TAPT），效果则最佳。





## 4. 预训练语言模型 - 后时代

![](http://ww1.sinaimg.cn/large/006gOeiSly1gdb1uomc8vj30m70re79a.jpg)

首先，我们来分析一下这张图，从上往下：

- **Contextual：** 谈论 静态 embedding 与 上下文 embedding，被做烂了，pass。
- **Architectures：** 模型整体架构，这部分还有的探讨，可以参见上面 T5 的Model Architecture 部分。 目前业界还没有统一的标准说应该选择哪种架构，不过从 T5 的效果来看， Transformer+Encoder+Decoder 的效果是最好的，但参数量也上去了。其实就目前来看，研究的意义不是很大了，除非说能出现一个大的突破。
- **Task Types：** 谈论了两件事： **语言模型的选择**以及**Contrastive Learning**，其实这两个应该分开讨论。
- **Multi-Lingual：**  从多国语言的角度出发，这方面不太懂，也不感兴趣，觉得用处不会太大。
- **Multi-Modal：** 多模角度，我个人认为这对于工业界是十分有意义的。
- **Knowledge Enriched：** 知识 + 预训练语言模型，我觉得这是一个很值得研究的方向，无论是在工业界和学术界。
- **Domain Specific：** 特定领域 + 预训练语言模型，我觉得这方面很有搞头，毕竟很多专有领域跟公共领域还是很不同的，比如医学，生物，法学等。由于每看过相关文章，无法说上面的模型与 bert在同样语料上预训练后哪个效果好，但还是有一定参考价值的。
- **Language-Specific：** 这块我觉得还是很有研究价值的，毕竟我们中文跟英文从各个方面来说差距还是蛮大的，如果能对语言有深入了解，感觉还有搞头。
- **Model Compression：** 模型压缩，这个在工业界用处很大，十分建议研究，需求也很大，一些蒸馏方法所需要的资源门槛也比较低，如果有资源，有idea，建议入坑。

考虑到涉及到的内容太多，我这里抽取四个部分讨论，分别是： Architectures， Task Types， Knowledge Enriched 以及 language generation。 

### 1. AE vs AR

AR 语言模型：自回归语言模型，指的是，依据前面（或后面）出现的 tokens 来预测当前时刻的 token， 代表有 ELMO， GPT 等。
$$
forward: p(x) = \prod_{t=1}^T p(x_t | x_{<t}) \\ backward: p(x) = \prod_{t=T}^1 p(x_t | x_{>t})
$$
AE 语言模型：通过**上下文信息**来预测被 mask 的 token， 代表有 BERT , Word2Vec(CBOW)  。
$$
p(x) = \prod_{x\in Mask} p(x|context)
$$
**AR 语言模型：**

- **缺点：**它只能利用单向语义而不能同时利用上下文信息。 ELMO 通过双向都做AR 模型，然后进行拼接，但从结果来看，效果并不是太好。
- **优点：** 对生成模型友好，天然符合生成式任务的生成过程。这也是为什么 GPT 能够编故事的原因。

**AE 语言模型：**

- **缺点：** 由于训练中采用了 [MASK] 标记，导致预训练与微调阶段不一致的问题。 此外对于生成式问题， AE 模型也显得捉襟见肘，这也是目前 BERT 为数不多实现大的突破的领域。
- **优点：** 能够很好的编码上下文语义信息， 在自然语言理解相关的下游任务上表现突出。

### 2. 预训练语言模型融入知识

ERINE , K-BERT

### 3. 预训练语言模型与自然语言生成



### 4. 多模态预训练 - 图像+文本

由于时间有限，这里分别介绍几个经典的和最新的model。

OSCAR

### 5. 多模态预训练 - 音频+文本



### 6. 中文可用的预训练语言模型





## 5. 如何预训练一个好的预训练语言模型





## Reference

**语言模型基础与词向量：**

[1] A Neural Probabilistic Language Model

[2] Mikolov, T.(2013). Distributed Representations of Words and Phrases and their Compositionality.

[3] Mikolov, T.(2013). Efficient Estimation of Word Representations in Vector Space.

[4] Rong, X. (2014). word2vec Parameter Learning Explained.

**预训练语言模型：**

[1] ELMO: Deep contextualized word representations

[2]  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

[3] GPT 1.0: Improving Language Understanding by Generative Pre-Training

[4] GPT 2.0: Language Models are Unsupervised Multitask Learners

[5] GPT 3.0: Language Models are Few-Shot Learners





**应用预训练语言模型：**

[1]  To tune or not to tune? adapting pretrained representations to diverse tasks. 

[2]  Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks





**预训练语言模型 - 后时代：**

[2]  ERNIE - Enhanced Language Representation with Informative Entities

[3]  ERNIE - Enhanced Representation through Knowledge Integration

[4]  ERNIE 2.0 - A Continual Pre-training Framework for Language Understanding

[5]  MASS - Masked Sequence to Sequence Pre-training for Language Generation

[6] UNILM - Unified Language Model Pre-training for Natural Language Understanding and Generation

[7]  XLNet - Generalized Autoregressive Pretraining for Language Understanding

[8]  RoBERTa - A Robustly Optimized BERT Pretraining Approach

[9] TransformerXL: Attentive Language Models Beyond a Fixed-Length Context



**如何预训练一个好的预训练语言模型：**

[1] Pre-trained Models for Natural Language Processing: A Survey

[2] T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer



```
Bag of Tricks for Efficient Text Classification
```

