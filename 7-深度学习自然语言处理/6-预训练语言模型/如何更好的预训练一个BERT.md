# 如何更好的预训练一个BERT







## Reference

Train No Evil- Selective Masking for Task-Guided Pre-Training

Don’t Stop Pretraining- Adapt Language Models to Domains and Task

[Recent Advances in Language Model Fine-tuning](https://ruder.io/recent-advances-lm-fine-tuning/)

[Reducing Toxicity in Language Models](https://lilianweng.github.io/lil-log/2021/03/21/reducing-toxicity-in-language-models.html)

