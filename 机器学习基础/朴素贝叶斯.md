# 朴素贝叶斯
tags: machine-learning

---

## 1. 先验概率与后验概率

### 1. 条件概率

$$
P(X|Y) =  \frac{P(X,Y)}{P(Y)}
$$

- $P(X|Y)$含义： 表示 y 发生的条件下 x 发生的概率。

### 2. 先验概率

- 含义： 表示事件发生前的预判概率。这个可以是基于历史数据统计，也可以由背景常识得出，也可以是主观观点得出。
- 一般都是单独事件发生的概率，如 P(A)

### 3. 后验概率

- 基于先验概率求得的**反向条件概率**，形式上与条件概率相同（若 `P(X|Y)` 为正向，则 `P(Y|X)` 为反向）

## 从贝叶斯公式谈起

贝叶斯公式如下：
$$
P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)}
$$
我们先来推导一下贝叶斯公式的由来。

首先，我们知道条件概率有：
$$
P(Y|X) = \frac{P(X,Y)}{P(X)}   \\\
P(X|Y) = \frac{P(X,Y) }{P(Y)}
$$
由此我们可以得出：
$$
P(Y, X) = P(X, Y) = P(Y|X)P(X) = P(X|Y) P(Y)
$$
从而我们得出了贝叶斯公式。

接下来我们谈谈贝叶斯公式中的分子，分母分别代表什么。对于贝叶斯公式，
$$
P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)}
$$
其中有：

> - P(Y) 叫做先验概率，意思是事件X发生之前，我们对事件Y发生的一个概率的判断
> - P(Y|X) 叫做后验概率，意思是时间X发生之后，我们对事件Y发生的一个概率的重新评估
> - P(Y,X) 叫做联合概率， 意思是事件X与事件Y同时发生的概率。

## 从机器学习视角理解贝叶斯

在机器学习中，我们可以将X理解为“具有某特征”， 而Y理解为“类别标签”，于是有：
$$
P("属于某类“ \, \, | \, \, "具有某特征" ) = \frac{P("具有某特征" | "属于某类") P("属于某类")}{P("具有某特征" )}
$$

## 举个小例子

垃圾邮件分类算是一个比较经典的问题了，我们这里就以此为例：

问题描述：

> - 判断 P("垃圾邮件" | "具有某特征") 的概率（也就是说判断一封邮件是否为垃圾邮件的概率）
>
> - 我们这里给出一封邮件，判断其是否为垃圾邮件：
>
>   > 我公司可办理正规发票17%增值税发票点数优惠。

问题解决：

> - 我们要求取的概率为：
>   $$
>   P("垃圾邮件”| “我公司可办理正规发票17\%增值税发票点数优惠。” )
>   $$
>
> - 第一步：首先，我们是要分词（NLP中最基础的技术之一），分词之后结果为：
>   $$
>   P("垃圾邮件”| (“我" , "公司" , "可", "办理", "正规", "发票", "17\%", "增值税", "发票", "点数", "优惠" 。”)  )   \\\
>   = \frac{P((“我" , "公司" , "可", "办理", "正规", "发票", "17\%", "增值税", "发票", "点数", "优惠" 。”)  | "垃圾邮件" )  P("垃圾邮件")}{P( (“我" , "公司" , "可", "办理", "正规", "发票", "17\%", "增值税", "发票", "点数", "优惠" 。”)  )}
>   $$
>
>

## 条件独立假设

在后面的朴素贝叶斯中主要使用了**条件独立假设**， 我们这里先简单介绍一下：
$$
P(x|c) = p(x_1, x_2,  \cdots x_n | c) = p(x_1 | c) * P(x_2 | c) \cdots P(x_n|c)
$$
于是，上式中分子的一部分可以变为：
$$
P((“我" , "公司" , "可", "办理", "正规", "发票", "17\%", "增值税", "发票", "点数", "优惠" 。”)  |S =  "垃圾邮件" ) \\\
= P("我" | S)  × P("司" | S) × \cdots  × P("优惠" | S)
$$


于是，问题就比较好求了，比如：
$$
P("我" | S) = \frac{垃圾邮件中“我” 出现的次数} {垃圾邮件中所有词出现的次数}
$$

## 条件独立假设与朴素贝叶斯

二者之间的关系可以通过一句话描述： **加上条件独立假设的贝叶斯方法就是朴素贝叶斯方法**。

这里注意一点的是，**朴素贝叶斯没有考虑到词之间的顺序，也就是说“武松打死老虎” 与 “老虎打死武松” 最终我们得出的概率是一样的。 **但是令人奇特的是即使它的缺陷如此明显，但是其在垃圾邮件分类以及诸多任务中表现特别的好。

## 朴素贝叶斯中的三种模型

### 1.  多项式模型

多项式模型适用于离散特征情况，在文本领域应用广泛， 其基本思想是：我们将重复的词语视为其出现多次。

比如：“我公司可办理正规发票17%增值税发票点数优惠。” 中发表出现了两次，那么则有：
$$
P( " 代开“， ”发票“， ”发票“， ”我“ | S) = P("代开" | S) P( ”发票“ | S)   P( ”发票“ | S) P("我" | S)
$$
我们看到，我们有两个发票出现，所以我们乘了两个 P("发票" | S)。

### 2. 高斯模型

https://blog.csdn.net/u012162613/article/details/48323777

http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/

高斯模型适合**连续特征情况**， 我们先给出高斯公式：
$$
P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2}  {2\sigma_{y_{k}}^{2}}   )
$$




### 3. 伯努利模型

> 伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次。
> $$
> P( " 代开“， ”发票“， ”发票“， ”我“ | S) = P("代开" | S)   P( ”发票“ | S) P("我" | S)
> $$
> 我们看到，”发票“出现了两次，但是我们只将其算作一次。