# 特征工程

tags: 机器学习

---

[TOC]

![8.数据清洗与特征处理](..\img\8.数据清洗与特征处理.jpg)

## 1. 如何处理数据中的缺失值

有些特征可能因为无法采样或者没有观测值而缺失，此时需要对这些缺失的特征值进行特殊处理。

### 1. 缺失值较多

如果该特征中的缺失值较多，则应该直接舍弃，否则反而可能引入较大噪声，造成反效果。

### 2. 缺失值较少

即当缺失值在 10% 以内时，我们可以采用多种方式处理：

- 将缺失值当一个特征处理，用 一个异常值表示， 如0

  ```
  data_train.fillna(0) 
  ```

- 用均值填充

  通常一个策略是取相同 label 的数据的均值

  ```
  data_train.fillna(data_train.mean()) 
  ```

- 以上下数据填充

  ```
  data_train.fillna(method='pad')  # 上一个数据填充
  data_train.fillna(method='bfill')  # 下一个数据填充
  ```

- 插值法

  ```
  data_train.interpolate() # 即估计中间点的值
  ```

- 用随机森林等算法拟合

  > 将数据分为有值和缺失值2份，对有值的数据采用随机森林拟合，然后对有缺失值的数据进行预测，用预测的值来填充。

## 2.  特征常见处理手段

### 1.数值归一化

归一化的目的是将所有的特征都统一到一个大致相同的数值区间中。

从梯度下降的角度来看，对于两个特征x1，x2， x1的范围远远大于x2，在学习率相同的情况下，x1的更新速度会大于x2，需要较多的迭代才能得到最优解。

![](http://ww1.sinaimg.cn/large/006gOeiSly1g0r7bxl5kfj30ie0a840y.jpg)

- 函数归一化： min-max 归一化， 零均值归一化（参考 **Normalization** 一节）
- 分维度归一化
- 排序归一化

### 2. 离散化

连续值的离散化：

- **等值划分：**将特征按照值域进行均分，每一段内的取值等同处理。例如某个特征的取值范围为[0，10]，我们可以将其划分为10段，[0，1)，[1，2)，...，[9，10)。
- 等量划分是根据样本总数进行均分，每段等量个样本划分为1段。

区别：例如距离特征，取值范围［0，3000000］，现在需要切分成10段，如果按照等比例划分的话，会发现绝大部分样本都在第1段中。使用等量划分就会避免这种问题，最终可能的切分是[0，100)，[100，300)，[300，500)，..，[10000，3000000]，前面的区间划分比较密，后面的比较稀疏。

## 3. 共线性问题  -- TODO

对于回归算法而言， 其首先假设回归模型的解释变量之间不存在线性关系， 即解释变量X1，X2，……，Xk中的任何一个都不能是其他解释变量的线性组合。如果违背这一假定，即线性回归模型中某一个解释变量与其他解释变量间存在线性关系，就称线性回归模型中存在多重共线性。

多重共线性违背了解释变量间不相关的古典假设，将给普通最小二乘法带来严重后果。

其实，简单来说，就是特征冗余，容易导致过拟合。

### 1. 如何判定共线性问题？

- 相关性分析。当相关性系数高于0.8，表明存在多重共线性；但相关系数低，并不能表示不存在多重共线性
- 方差膨胀因子VIF。当VIF大于5或10时，代表模型存在严重的共线性问题；
- 条件系数检验。 当条件数大于100、1000时，代表模型存在严重的共线性问题。

### 2. 如何消除共线性问题？

通常可通过PCA降维、逐步回归法和LASSO回归等方法消除共线性。



## 4. 如何进行特征选择？

### 1. 特征分类

- 相关特征： 对于特定的任务和场景有一定帮助的属性，这些属性能有效提升算法性能。
- 无关特征：在特定的任务和场景下完全无用的属性，这些属性对对象在本目标环境下完全无用。
- 冗余特征：同样是在特定的任务和场景下具有一定帮助的属性，但这类属性已过多的存在，不具有产生任何新的信息的能力。

### 2. 如何考虑特征选择

可以从以下两个方面来选择特征：

- 特征是否具有发散性：某个特征若在所有样本上的都是一样的或者接近一致，即方差非常小。 也就是说所有样本的都具有一致的表现，那这些就不具有任何信息。
- 特征与目标的相关性：与目标相关性高的特征，应当优选选择。

### 3. 特征选择方法分类

- 过滤法：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。  
- 包装法：根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。  
- 嵌入法：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。

### 4. 特征选择目的

- 减少特征维度，使模型泛化能力更强，减少过拟合;  
- 降低任务目标的学习难度；
- 一组优秀的特征通常能有效的降低模型复杂度，提升模型效率 



