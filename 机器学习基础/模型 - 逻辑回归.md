# 机器学习： 逻辑回归

tags: machine-learning

---

[TOC]

## logistic回归简介

logistic回归用于解决的是分类问题，**其基本思想是：根据现有数据对分类边界线建立回归公式,以此进行分类。**也就是说，logistic 回归不是对所有数据点进行拟合，而是要对**数据之间的分界线**进行拟合。

- 逻辑回归的本质： 极大似然估计
- 逻辑回归的激活函数：Sigmoid
- 逻辑回归的代价函数：交叉熵

## Logistic 回归的数学表达

$$
h_\theta(x) = sigmoid(\theta^T X)  = \frac{1}{1 + e^{-\theta^T X}}
$$
##  如何求解最优的 $\theta$

首先，我们依旧是要找到一个合适的损失函数，在Logistic回归中的损失函数为：
$$
J(\theta) =    - \frac{1}{m}   \left[  \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)}))   + (1-y^{(i)}) log(1 - h_\theta(x^{(i)}))             \right]
$$
我们最终给它加一个正则化项：
$$
J(\theta) =    - \frac{1}{m}   \left[  \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)}))   + (1-y^{(i)}) log(1 - h_\theta(x^{(i)}))             \right] + \frac{\lambda}{2m} \sum_{j=1}^{m}\theta_j^2
$$
最后，我们要求最优参数的话，依旧是使用梯度下降算法来获取$J(\theta)$ 的最小值时对应的参数。

---

## 常见问题

### 1. 逻辑回归与线性回归

- 逻辑回归处理分类问题，线性回归处理回归问题

- 线性回归的拟合函数本质上是对 **输出变量 y 的拟合**， 而逻辑回归的拟合函数是对 **label 为1的样本的概率的拟合**。
  $$
  线性回归：f(x)=\theta ^{T}x \\
  逻辑回归：f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)， \quad g(z)=\frac{1}{1+e^{-z}}
  $$

- 线性回归其参数计算方式为**最小二乘法**， 逻辑回归其参数更新方式为**极大似然估计**。

- 线性回归更容易受到异常值的影响， 而LR对异常值有较好的稳定性。

### 2. 推导一下 LR

- sigmoid :
  $$
  g(z) = \frac{1}{1+e^{-z}} \\
  g'(z) = g(z)(1-g(z))
  $$

- LR 的定义：
  $$
  h_{\theta}(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}
  $$

- LR 满足**伯努利分布：**
  $$
  P(Y=1|x; \theta) = h_{\theta}(x) \\
  P(Y=0|x; \theta)  = 1 - h_{\theta}(x) \\
  p(y|x; \theta) = (h_{\theta}(x))^y (1-h_{\theta}(x))^{1-y}
  $$

- **损失函数（极大似然）:**  对于训练数据集，特征数据 $x={x_1, ...x_m}$ 和其对应的分类标签 $y = {y_1,...y_m}$ ， 假设 m 个样本是相互独立的，那么极大似然函数为： 
  $$
  \begin{align}
  L(\theta) &= \prod_{i=1}^m p(y^{(i)}|x(i);\theta) \\ 
  &= \prod_{i=1}^m  (h_{\theta}(x^{(i)}))^{y^{(i)}} (1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}\\
  \end{align}
  $$
  那么它的 log 似然为：
  $$
  \begin{align}
  L(\theta) &= log L(\theta ) \\
  &= \sum_{i=1}^m y^{(i)} log h(x^{(i)}) + (1-y^{(i)}) log (1-h(x^{(i)}))
  \end{align}
  $$

- 参数优化（梯度上升）
  $$
  \begin{align}
  \frac{\partial L(\theta)}{\partial \theta_j} &= (y \frac{1}{g(\theta^Tx)} - (1-y) \frac{1}{1 -g(\theta^Tx)}) \frac{\delta g(\theta^Tx)}{\delta \theta_j} \\
  &= (y \frac{1}{g(\theta^Tx)} - (1-y)\frac{1}{1 -g(\theta^Tx)} ) g(\theta^Tx)(1-g(\theta^Tx)) \frac{\delta \theta^Tx}{\theta_j} \\
  &= (y (1 - g(\theta^Tx)) - (1-y) g(\theta^Tx)) x_j \\
  &= [y - h_{\theta} (x)]x_j \\
  \end{align}
  $$



$$
\begin{align}
\theta_j &= \theta_j + \alpha \frac{\partial L(\theta)}{\partial \theta} \\
&= \theta_j + \alpha [y^{(i)} - h_{\theta} (x^{(i)})]x_j^{(i)}   
\end{align}
$$



- 损失函数：
  $$
  J(\theta) = - \frac{1}{m}   \left[  \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)}))   + (1-y^{(i)}) log(1 - h_\theta(x^{(i)}))             \right]
  $$



### 3. LR 如何实现多分类？

- **方式1：** 修改逻辑回归的损失函数，使用softmax函数构造模型解决多分类问题，softmax分类模型会有相同于类别数的输出，输出的值为对于样本属于各个类别的概率，最后对于样本进行预测的类型为概率值最高的那个类别。
- **方式2：** 根据每个类别都建立一个二分类器，本类别的样本标签定义为0，其它分类样本标签定义为1，则有多少个类别就构造多少个逻辑回归分类器。

若所有类别之间有明显的互斥则使用softmax分类器，若所有类别不互斥有交叉的情况则构造相应类别个数的逻辑回归分类器。

