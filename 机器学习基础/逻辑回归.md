# 机器学习： 逻辑回归

tags: machine-learning

---

## logistic回归简介

logistic回归用于解决的是分类问题，**其基本思想是：根据现有数据对分类边界线建立回归公式,以此进行分类。**也就是说，logistic回归不是对所有数据点进行拟合，而是要对**数据之间的分界线**进行拟合。

logistic 回归可用于二分类，也可用于多分类，但是二分类更加常用。


## Logistic 回归的数学表达

$$
H_\theta(x) = sigmoid(\theta^T X)  = \frac{1}{1 + e^{-\theta^T X}}
$$
$$
\theta = [\theta_0, \theta_1, \cdots, \theta_n]   \\\
X  = [x_0, x_1, \cdots , x_n]
$$

##  如何求解最优的 $\theta$

首先，我们依旧是要找到一个合适的损失函数，在Logistic回归中的损失函数为：
$$
J(\theta) =    - \frac{1}{m}   \left[  \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)}))   + (1-y^{(i)}) log(1 - h_\theta(x^{(i)}))             \right]
$$
接下来我解释一下为什么选择这个损失函数，我们试着将其拆开来，它其实是这样的：
$$
Cost(h_\theta(x), y)  =     -log(h_\theta(x))   \qquad   if \quad  y = 1   \\\
Cost(h_\theta(x), y)  =     -log(1 - h_\theta(x))   \qquad   if \quad  y = 0
$$
这样，对于任意一个误分类点，就会造成一个很高的损失(Sigmoid的特性)。更巧妙的是，我们在最终的损失函数前面分别乘了一个  $y$ 与 $1-y$  ， 这样当 $y = 1 $ 时，第二项就没了，当 $y=0$ 的时候，第一项就没了，这样其实就是我们前面两个$ Cost$ 求和。

我们最终给它加一个正则化项：
$$
J(\theta) =    - \frac{1}{m}   \left[  \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)}))   + (1-y^{(i)}) log(1 - h_\theta(x^{(i)}))             \right] + \frac{\lambda}{2m} \sum_{j=1}^{n}\theta_j^2
$$
最后，我们要求最优参数的话，依旧是使用梯度下降算法来获取$J(\theta)$ 的最小值时对应的参数。

## logistic 回归中的正则化

正则化在 logistic 回归中极为重要，如果没有正则化，逻辑回归的渐近性会不断促使损失在高维度空间内达到 0。因此，大多数逻辑回归模型会使用以下两个策略之一来降低模型复杂性：

- L2 正则化。
- 早停法，即，限制训练步数或学习速率。

---

## 常见问题

### 1. 逻辑回归于线性回归

- 逻辑回归处理分类问题，线性回归处理回归问题

- 线性回归的拟合函数本质上是对输出变量 y 的拟合， 而逻辑回归的拟合函数是对 label 为1的样本的概率的拟合。
  $$
  线性回归：f(x)=\theta ^{T}x=\theta _{1}x _{1}+\theta _{2}x _{2}+...+\theta _{n}x _{n} \\
  逻辑回归：f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)， \quad g(z)=\frac{1}{1+e^{-z}}
  $$

- 线性回归其参数计算方式为最小二乘法， 逻辑回归其参数更新方式为极大似然估计

### 2. 推导一下 LR

把握三个基本点：

- LR 的定义: 线性回归 + sigmoid
  $$
  P(Y=1|x) = \frac{1}{1+e^{-wx}} = \sigma{(x)} \\
  P(Y=1|x)  = 1 - \sigma{(x)}
  $$
  

- 损失函数（极大似然）: 对数损失函数,由极大似然推得:
  $$
  L(w) = -log(\prod_{i=1}^N [\sigma(x_i)]^{y_i}[1-\sigma(x_i)]^{1-y_i})\\ = -\sum_{i=1}^N [y_i log \, \sigma(x_i) + (1 - y_i)log(1 - \sigma(x_i))] \\
  $$
  将 $\sigma(x)$ 带入得到:
  $$
  L(w) = - \sum_{i=1}^N [y_i(wx_i)] - log(1 + e^{wx_i})
  $$
  

- 参数优化（梯度下降）
  $$
  \frac{\partial L(w)}{\partial w} = - \sum_{i=1}^N [y_ix_i - \frac{e^{wx_i}}{1 + e^{wx_i}}]
  $$





