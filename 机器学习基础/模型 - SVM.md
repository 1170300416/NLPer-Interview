# SVM：支持向量机

tags: 机器学习

---

## 简介

SVM 三宝： 间隔，对偶，核技巧，判别模型。

- 支持向量：在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。
- 支持向量机（Support Vector Machine，SVM）：其含义是通过**支持向量**运算的分类器。

SVM 是一种**二分类模型**， 它的目的是寻找一个超平面来对样本进行分割，分割的依据是**间隔最大化**，最终转化为一个凸二次规划问题来求解。

## 什么是支持向量？

**训练数据集中与分离超平面距离最近的样本点的实例成为支持向量**

通俗理解：

- 数据集种的某些点，位置比较特殊。比如 `x+y-2=0` 这条直线，假设出现在直线上方的样本记为 A 类，下方的记为 B 类。
- 在寻找找这条直线的时候，一般只需看两类数据，它们各自最靠近划分直线的那些点，而其他的点起不了决定作用。
- 这些点就是所谓的“支持点”，在数学中，这些点称为**向量**，所以更正式的名称为“**支持向量**”。

## SVM 能解决哪些问题？

- **线性分类：**对于n维数据，SVM 的目标是找到一个 n-1 维的最佳超平面来将数据分成两部分。 通过增加一个约束条件： **要求这个超平面到每边最近数据点的距离是最大的。**
- **非线性分类：** SVM通过结合使用**拉格朗日乘子法**和KTT条件，以及**核函数**可以生产线性分类器

## 支持向量机的分类

- 软间隔SVM(线性SVM)：当训练数据接近线性可分时，通过软间隔最大化，学习一个线性分类器。
- 非线性SVM： 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性SVM。

### 1. 硬间隔SVM

硬间隔SVM（线性可分SVM)： 当训练数据可分时，通过硬讲个最大化，学习一个线性表分类器。

分离超平面可定义为：
$$
w^Tx + b = 0
$$
空间中任意点 x 到超平面(w, b) 的距离为：
$$
r = \frac{|w^Tx + b|}{||w||}
$$
对于该分离超平面的分离结果有：
$$
\begin{cases} w^Tx_i + b >= +1, & y_i = +1 \\ w^Tx_i + b <= -1, & y_i = -1 \end{cases}
$$


### 2. 软间隔SVM





## 核函数和核技巧

### 1. 核函数的作用

- 目的： 将原坐标系中线性不可分数据通过核函数映射到另一空间，尽量使数据在新的空间里线性可分。

- 特点：

  > - 

### 2. 常见核函数

| 核函数         | 表达式                                                       | 备注                                |
| -------------- | ------------------------------------------------------------ | ----------------------------------- |
| 线性核         | $k(x,y)=x^{t}y+c$                                            |                                     |
| 多项式核       | $k(x,y)=(ax^{t}y+c)^{d}$                                     | $d\geqslant1$为多项式的次数         |
| 指数核         | $k(x,y)=exp(-\frac{\left \|x-y \right \|}{2\sigma ^{2}})$    | $\sigma>0$                          |
| 高斯核         | $k(x,y)=exp(-\frac{\left \|x-y \right \|^{2}}{2\sigma ^{2}})$ | $\sigma$为高斯核的带宽，$\sigma>0$, |
| 拉普拉斯核     | $k(x,y)=exp(-\frac{\left \|x-y \right \|}{\sigma})$          | $\sigma>0$                          |
| ANOVA Kernel   | $k(x,y)=exp(-\sigma(x^{k}-y^{k})^{2})^{d}$                   |                                     |
| Sigmoid Kernel | $k(x,y)=tanh(ax^{t}y+c)$                                     | $tanh$为双曲正切函数，$a>0,c<0$     |



## QA

### 1. SVM 中的支持向量是什么意思？

![10.svm](..\img\10.svm.png)

如上图所示，我们在获得分离超平面时，并非所有的点都对分离超平面的位置起决定作用。其实在特别远的区域，哪怕你增加10000个样本点，对于超平面的位置，也是没有作用的，因为分割线是由几个关键点决定的（图上三个），这几个关键点支撑起了一个分离超平面，所以这些关键点，就是**支持向量**。