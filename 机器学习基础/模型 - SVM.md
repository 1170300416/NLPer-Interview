# SVM：支持向量机

tags: 机器学习

---

[TOC]

## 简介

https://zhuanlan.zhihu.com/p/77750026

SVM 三宝： **间隔，对偶，核技巧**。它属于判别模型。Support Vector Machine

- **支持向量：**在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。
- **支持向量机（SVM）**：其含义是通过**支持向量**运算的分类器。

SVM 是一种**二分类模型**， 它的目的是寻找一个超平面来对样本进行分割，分割的依据是**间隔最大化**，最终转化为一个凸二次规划问题来求解。

## 基础准备

### 1. 线性可分

$D_0$ 和 $D_1$ 是 n 维空间中的两个点集， 如果存在 n 维向量 $w$ 和实数 $b$ ， 使得：
$$
wx_i +b > 0; \quad x_i \in D_0 \\
wx_j + b < 0; \quad x_j \in D_1
$$
则称 $D_0$ 与 $D_1$ 线性可分。

### 2. 最大间隔超平面

能够将 $D_0$  与 $D_1$ 完全正确分开的 $wx+b = 0$ 就成了一个超平面。

为了使得这个超平面更具鲁棒性，我们会去找最佳超平面，以最大间隔把两类样本分开的超平面，也称之为**最大间隔超平面**。

- 两类样本分别分割在该超平面的两侧
- 两侧距离超平面最近的样本点到超平面的距离被最大化了

### 3. 什么是支持向量？

**训练数据集中与分离超平面距离最近的样本点成为支持向量**

![1](..\img\SVM\1.jpg)

### 4. SVM 能解决哪些问题？

- **线性分类：**对于n维数据，SVM 的目标是找到一个 n-1 维的最佳超平面来将数据分成两部分。 

  通过增加一个约束条件： **要求这个超平面到每边最近数据点的距离是最大的。**

- **非线性分类：** SVM通过结合使用**拉格朗日乘子法**和KTT条件，以及**核函数**可以生产线性分类器

### 5. 支持向量机的分类

- **硬间隔SVM（线性可分SVM)**： 当训练数据可分时，通过间隔最大化，学习一个线性表分类器。
- **软间隔SVM(线性SVM)**：当训练数据接近线性可分时，通过软间隔最大化，学习一个线性分类器。
- **Kernel SVM**： 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性SVM。

## 硬间隔 SVM

### 1. SVM 最优化问题

任意分离超平面可定义为：
$$
w^Tx + b = 0
$$
二维空间中点 $(x,y)$ 到直线 $Ax + By + C=0$ 的距离公式为：
$$
\frac{|Ax+ By + C}{\sqrt{A^2 + B^2}}
$$


扩展到n维空间中，任意点 $x$ 到超平面$w^Tx + b = 0$ 的距离为：
$$
\frac{|w^Tx + b|}{||w||} \\
||w|| = \sqrt{w_1^2 + ... + w_n^2}
$$
假设，支持向量到超平面的距离为 $d$ ，那么就有：
$$
\begin{cases} \frac{w^Tx + b}{||w||} \geq d, & y_i = +1 \\ \frac{w^Tx + b}{||w||} \leq -d, & y_i = -1 \end{cases}
$$
稍作转化可得到：
$$
\begin{cases} \frac{w^Tx + b}{||w||d} \geq 1, & y_i = +1 \\ \frac{w^Tx + b}{||w||d} \leq -1, & y_i = -1 \end{cases}
$$
考虑到 $||w||d$ 为正数，我们暂且令它为 1（之所以令它等于 1，是为了方便推导和优化，且这样做对目标函数的优化没有影响）：
$$
\begin{cases} w^Tx + b >= +1, & y_i = +1 \\ w^Tx + b <= -1, & y_i = -1 \end{cases}
$$

两个方程合并，则有：
$$
y_i (w^Tx + b) \geq 1
$$
那么我们就得到了最大间隔超平面的上下两个超平面：

![2](..\img\SVM\2.jpg)

每个**支持向量**到超平面的距离为：
$$
d = \frac{|w^Tx + b|}{||w||}
$$
我们的目的是最大化这个距离：
$$
max \quad 2 * \frac{|w^Tx + b|}{||w||}
$$
在支持向量确定以后， $w^Tx + b$ 是个常数，所以可以转化为：
$$
max \quad \frac{2}{||w||}
$$
可以转化为：
$$
min \quad \frac{1}{2} ||w||
$$
为了方便计算，我们去除 $||w||$ 的根号，则有：
$$
min \quad \frac{1}{2} ||w||^2
$$
那么我们的最优化问题为：
$$
min \quad \frac{1}{2} ||w||^2 \quad st. y_i(w^Tx + b) \geq 1
$$

### 2. 对偶问题

#### 1. 拉格朗日乘数法 - 等式约束优化问题

高等数学中，其等式约束优化问题为：
$$
min f(x_1, ..., x_n) \quad  st. \quad h_k(x_1, ... , x_n) = 0
$$
那么令：
$$
L(x, \lambda) = f(x) + \sum_{k=1}^l \lambda_k h_k(x)
$$

- $L(x, \lambda)$ ： Lagrange 函数
- $\lambda$ ： Lagrange 乘子，没有非负要求

利用必要条件找到可能的极值点，我们得到如下的方程组：
$$
\begin{cases} 
\frac{\delta L}{ \delta x_i} = 0, & i=1,2,...,n \\ 
\frac{\delta L}{ \delta \lambda_k} = 0, & k=1,2,...,l
\end{cases}
$$
等式约束下的Lagrange 乘数法引入了 $l$ 个 Lagrange 乘子，我们将 $x_i$ 与 $\lambda_k$ 一视同仁，将$\lambda_k$ 也看做优化变量，那么共有 $(n+l)$ 个优化变量。

#### 2. 拉格朗日乘数法 - 不等式约束优化问题

对于不等式约束优化问题，其主要思想在于**将不等式约束条件转变为等式约束条件，引入松弛变量，将松弛变量也是为优化变量。**

![3](..\img\SVM\3.jpg)

对于我们的问题：
$$
min \quad \frac{1}{2} ||w||^2 \quad st. \quad  g_i(w) = 1- y_i(w^Tx + b) \leq 0
$$
引入松弛变量 $a_i^2$ 得到：
$$
h_i(w, a_i) = g_i(w) + a_i^2 = 0
$$
这里加平方主要为了不再引入新的约束条件，如果只引入 $a_i$ 那我们必须要保证 $a_i \geq 0$ 才能保证 $h_i(w, a_i)$ ，这不符合我们的意愿。

此时，我们就将不等式约束转化为等式约束，并得到 Lagrange 函数：
$$
\begin{align}
L(w, \lambda, a) &= \frac{1}{2} f(w) + \sum_{i=1}^n \lambda_i h_i(w) \\
&= \frac{1}{2} f(w) + \sum_{i=1}^n \lambda_i [g_i(w) + a_i^2] \quad \lambda_i \geq 0
\end{align}
$$
那么我们得到方程组有：
$$
\begin{cases} 
\frac{\delta L}{ \delta w_i} =\frac{\delta f}{\delta w_i} + \sum_{i=1}^n \lambda_i \frac{\delta g_i}{\delta w_i} =0 \\ 
\frac{\delta L}{ \delta a_i} = 2 \lambda_i a_i =0 \\
\frac{\delta L}{ \delta \lambda_i}=g_i(w) + a_i^2 = 0 \\
\lambda_i \geq 0
\end{cases}
$$
针对 $\lambda_i a_i = 0$ 有两种情况：

-  $\lambda_i = 0, a_i \neq 0$：此时约束条件 $g_i(w)$ 不起作用且 $g_i(w) < 0$ 

- $\lambda_i \neq 0, a_i = 0$： 此时 $g_i(w)=0, \lambda_i > 0$， 可以理解为约束条件 $g_i(w)$ 起作用了， 且$g_i(w) = 0$

综合可得：$\lambda_ig_i(w) = 0$， 且在约束条件起作用时 $\lambda_i > 0, g_i(w) = 0 $； 约束不起作用时， $\lambda_i = 0, g_i(w) < 0 $。

此时，方程组转化为：
$$
\begin{cases} 
\frac{\delta L}{ \delta w_i} =\frac{\delta f}{\delta w_i} + \sum_{i=1}^n \lambda_i \frac{\delta g_i}{\delta w_i} =0 \\ 
\lambda_ig_i(w) = 0 \\
g_i(w) \leq 0 \\
\lambda_i \geq 0
\end{cases}
$$
以上便是不等式约束优化优化问题的 **KKT(Karush-Kuhn-Tucker) 条件**， $\lambda_i$ 称为 KKT 乘子。







### 2. 软间隔SVM

软间隔允许部分样本点不满足约束条件：
$$
1 - y_i (w^Tx_i + b) \leq 0
$$




## 3. Kernel SVM

### 1. 思想

对于在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到支持向量机，就是非线性 SVM。

用 x 表示原来的样本点，用 $\phi(x)$ 表示 x 映射到新特征空间后的新向量。那么分割超平面可以表示为：
$$
f(x) = w \phi (x) + b
$$
此时，非线性 SVM 的对偶问题转化为：
$$
min_{\lambda} [\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^n  ]
$$


### 2. 核函数的作用

- 目的： 将原坐标系中线性不可分数据通过核函数映射到另一空间，尽量使数据在新的空间里线性可分。


### 2. 常见核函数

- 线性核函数：
  $$
  K(x_i, x_j) 
  $$
  



## QA

### 1. SVM 中的支持向量是什么意思？

![10.svm](..\img\10.svm.png)

如上图所示，我们在获得分离超平面时，并非所有的点都对分离超平面的位置起决定作用。其实在特别远的区域，哪怕你增加10000个样本点，对于超平面的位置，也是没有作用的，因为分割线是由几个关键点决定的（图上三个），这几个关键点支撑起了一个分离超平面，所以这些关键点，就是**支持向量**。

### 2. SVM 什么时候用线性核？

当数据的特征提取的较好，所包含的信息量足够大，很多问题是线性可分的那么可以采用线性核。

### 3. SVM 什么时候用高斯核？

若特征数较少,样本数适中,对于时间不敏感,遇到的问题是线性不可分的时候可以使用高斯核来达到更好的效果。

### 4. 什么是SVM ？

支持向量机为一个二分类模型，它的基本模型定义为特征空间上的间隔最大的线性分类器。而它的学习策略为最大化分类间隔，最终可转化为凸二次规划问题求解。

### 5. SVM 与 LR 的区别

- LR是参数模型，SVM为非参数模型。

  R采用的损失函数为logisticalloss，而SVM采用的是hingeloss。
- 在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。LR的模型相对简单，在进行大规模线性分类时比较方便。

### 6. SVM 作用与基本实现原理

- 作用： SVM可以用于解决二分类或者多分类问题。
- 原理： SVM的目标是寻找一个最优化超平面在空间中分割两类数据，这个最优化超平面需要满足的条件是：离其最近的点到其的距离最大化，这些点被称为支持向量。

### 7. 推到一下 SVM

#### 1. 模型定义

$$
max \, margin(w,b) \quad st. y_i(w^Tx_i+b) > 0 \\
$$

- 支持向量到超平面的距离为**样本点到分类超平面的最小距离**：

$$
\begin{align}
margin(w,b) &= min_{w,b,x_i}  distance(w,b,x_i) \\
&= min_{w,b,x_i} \frac{1}{||w||} |w^Tx_i + b|
\end{align}
$$

- 我们的目的是最大化支持向量到分类超平面的距离：
  $$
  max_{w,b} \, min_{x_i} \frac{1}{||w||} |w^Tx_i + b| \quad st. \, y_i(w^Tx_i+b) > 0\\
  := max_{w,b} \, min_{x_i} \frac{1}{||w||} y_i(w^Tx_i + b) \\
  := max_{w,b}  \frac{1}{||w||} \, min_{x_i} y_i(w^Tx_i + b)
  $$

- 对于  $y_i(w^Tx_i+b) > 0$ ， 存在 $r > 0$ 使得 $min_{x_i, y_i} y_i(w^Tx_i+b)  = r$。 我们可以直接设置 $r = 1 $ （因为 w，b 是可以随意缩放的）， 对整个等式是没有影响的。 此时转化为：
  $$
  max_{w,b} \frac{1}{||w||} \quad st. \quad min \, y_i(w^Tx_i + b) = 1 \\
  \implies \quad min_{w,b} \frac{1}{2} w^Tw \quad st. \, y_i(w^Tx_i + b) \geq 1; \, i = 1, ...,n
  $$

#### 2. 模型求解





### 8. SVM 软间隔与硬间隔表达式

- 硬间隔：
  $$
  min_{w,b} \frac{1}{2} ||w||^2 \qquad st. \quad y^{(i)}(w^Tx^{(i)} + b) \geq 1
  $$

- 软间隔：
  $$
  min_{w,b} \frac{1}{2} ||w||^2 + C \sum_{i=1}^m \xi_i  \qquad st. \quad y^{(i)}(w^Tx^{(i)} + b) \geq 1 \quad \xi_i \geq 0
  $$

### 9. SVM 使用对偶计算的目的，推到一下？

- 目的：

  > 1. 方便核函数的引入
  > 2. 原问题的求解复杂度与特征的维数相关，而转成对偶问题后只与问题的变量个数有关。

  由于SVM的变量个数为支持向量的个数，相较于特征位数较少，因此转对偶问题。通过拉格朗日算子发使带约束的优化目标转为不带约束的优化函数，使得W和b的偏导数等于零，带入原来的式子，再通过转成对偶问题。

### 10 . SVM 的物理意义？

构造一个最优化的超平面在空间中分割数据。

