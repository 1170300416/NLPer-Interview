# 机器学习： 线性回归

tags: machine-learning

---

## 线性回归简介

简单来说，线性回归算法就是**找到一条直线（一元线性回归）或一个平面（多元线性回归）能够根据输入的特征向量来更好的预测输出y的值。**其本质含义在于 X 与 Y 是线性相关的。
$$
y = \theta_0 + \theta_1x_1 + \cdots  + \theta_px_p  = \theta^Tx
$$

## 线性回归如何训练？

再线性回归中， 我们可以通过两种方法来求取参数 $\theta$ ， 一种是采用**正规方程**， 一种是采用**梯度下降方法**。 

### 1. 损失函数

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2, \qquad  \\ 或 \\ 矩阵表示: J(\theta) = \frac{1}{2m} (X\theta-y)^T(X\theta - y)
$$

### 2. 正规方程

我们使用 $J(\theta) $对 $\theta$ 求导， 得到：
$$
\frac{\delta J(\theta)}{\delta \theta} = 2 X^T(X\theta - y)
$$
令上式为0，我们可以得到 $ \theta$  的值为：
$$
\theta = (X^TX)^{-1}X^Ty
$$
我们可以直接通过矩阵运算来求出参数 $\theta$  的解。 而上式我们发现其涉及到了矩阵的可逆问题，**如果 $(X^TX)^{-1} $可逆，那么参数 $\theta$ 的解唯一**。 **如果不可逆， 则此时就无法使用正规方程的方法来解。** 

### 3. 梯度下降法

我们可以采用批量梯度下降算法， 此时有：

$$
\theta_j = \theta_j - \alpha \frac{\delta}{\delta \theta_j} J(\theta) \\ 带入J(\theta) 得： \theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}  \\ 或矩阵表达：\theta_j = \theta_j + \alpha \frac{1}{m}(y-X\theta)^Tx_j
$$

### 4. 两种方法的比较

- 梯度下降中需要选择适当的学习率 $\alpha $

-  梯度下降法中需要多次进行迭代，而正规方程只需要使用矩阵运算就可以完成

- 梯度下降算法对多特征适应性较好，能在特征数量很多时仍然工作良好， 而正规方程算法复杂度为 $O(n^3) $，所以**如果特征维度太高（特别是超过 10000 维），那么不宜再考虑该方法。**

-  正规方程中矩阵需要可逆。

## LWR - 局部加权回归

在线性回归中， 由于最终拟合出来的曲线是一条直线，其拟合能力极为有限（也可以解释为线性回归所求的是具有最小均方误差的无偏估计），因此很容易造成欠拟合现象， 而针对这个问题，有人提出了局部线性回归(LWR)。

局部加权回归其思想很简单： 我们对一个输入 w 进行预测时，赋予了 x 周围点不同的权值，距离 x 越近，权重越高。整个学习过程中误差将会取决于 x 周围的误差，而不是整体的误差，这也就是局部一词的由来。

在LWR中， 其损失函数为：
$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^m w^{(i)} (h_\theta(x^{(i)}) - y^{(i)})^2 \\ 矩阵表示: J(\theta) = \frac{1}{2m} (X\theta-y)^TW(X\theta - y)
$$
此时，使用回归方程求得：

$$
\theta = (X^TWX)^{-1}X^TWy 
$$


而通常， $w^{(i)} $ 服从高斯分布， 在x周围指数型衰减;
$$
w^{(i)} = e^{- \frac{|x^{(i)} - x|}{2 k^2 }} 
$$

其中， **k 值越小，则靠近预测点的权重越大，而远离预测点的权重越小**。所以参数k的值决定了权重的大小。 

> - k越大权重的差距就越小，k越小权重的差距就很大，仅有局部的点参与进回归系数的求取，其他距离较远的权重都趋近于零。
> - 如果k去进入无穷大，所有的权重都趋近于1，W也就近似等于单位矩阵，局部加权线性回归变成标准的无偏差线性回归，会造成欠拟合的现象；
> - 当k很小的时候，距离较远的样本点无法参与回归参数的求取，会造成过拟合的现象。


