# 机器学习： 线性回归

tags: machine-learning

---

发现一个很强的up主，机器学习方面讲的很清楚：https://space.bilibili.com/97068901/channel/detail?cid=54167

## 线性回归简介

简单来说，线性回归算法就是**找到一条直线（一元线性回归）或一个平面（多元线性回归）能够根据输入的特征向量来更好的预测输出y的值。**其本质含义在于 X 与 Y 是线性相关的。
$$
y = \theta_0 + \theta_1x_1 + \cdots  + \theta_px_p  = \theta^Tx
$$

## 线性回归如何训练？

再线性回归中， 我们可以通过两种方法来求取参数 $\theta$ ， 一种是采用**正规方程**， 一种是采用**梯度下降方法**。 

### 1. 损失函数

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2, \qquad  \\ 或 \\ 矩阵表示: J(\theta) = \frac{1}{2m} (X\theta-y)^T(X\theta - y)
$$

### 2. 正规方程

我们使用 $J(\theta) $对 $\theta$ 求导， 得到：
$$
\frac{\delta J(\theta)}{\delta \theta} = 2 X^T(X\theta - y)
$$
令上式为0，我们可以得到 $ \theta$  的值为：
$$
\theta = (X^TX)^{-1}X^Ty
$$
我们可以直接通过矩阵运算来求出参数 $\theta$  的解。 而上式我们发现其涉及到了矩阵的可逆问题，**如果 $(X^TX)^{-1} $可逆，那么参数 $\theta$ 的解唯一**。 **如果不可逆， 则此时就无法使用正规方程的方法来解。** 

### 3. 梯度下降法

我们可以采用批量梯度下降算法， 此时有：

$$
\theta_j = \theta_j - \alpha \frac{\delta}{\delta \theta_j} J(\theta) \\ 带入J(\theta) 得： \theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}  \\ 或矩阵表达：\theta_j = \theta_j + \alpha \frac{1}{m}(y-X\theta)^Tx_j
$$

### 4. 两种方法的比较

- 梯度下降中需要选择适当的学习率 $\alpha $

-  梯度下降法中需要多次进行迭代，而正规方程只需要使用矩阵运算就可以完成

- 梯度下降算法对多特征适应性较好，能在特征数量很多时仍然工作良好， 而正规方程算法复杂度为 $O(n^3) $，所以**如果特征维度太高（特别是超过 10000 维），那么不宜再考虑该方法。**

-  正规方程中矩阵需要可逆。

## 岭回归 - 正则化

前面通过正规方程得到的 w 的估计：
$$
\hat{w} = (X^TX)^{-1}X^Ty
$$
但是，当我们有 N 个样本，每个样本有 $x_i \in R^p$， 当 N < p 时， $X^TX$ 不可逆， 容易造成过拟合。

岭回归通过在矩阵 $X^TX$ 上加一个 $\lambda I$ 来使得矩阵可逆， 此时的 w 的估计：
$$
\hat{w} = (X^TX + \lambda I)^{-1}X^Ty
$$
而岭回归本质上是对 $L(w)$  进行 L2 正则化， 此时的 $L(w)$ 表示为：
$$
\begin{align}
J(w) &= \sum_{i=1}^N ||w^Tx_i - y_i ||^2 + \lambda w^Tw \\
&= (w^TX^T - Y^T)(Xw + Y) + \lambda w^Tw \\
&= w^TX^TXw - 2w^TX^TY  - Y^TY + \lambda w^Tw \\
&= w^T(X^TX + \lambda I)w - 2w^TX^TY - Y^TY
\end{align}
$$
那么对 $w$ 的极大似然估计有：
$$
\hat{w} = argmax \, J(w) \\
\frac{\delta J(w)}{\delta w} = 2(X^TX + \lambda I)w - 2 X^TY = 0
$$
那么我们就解得：
$$
\hat{w} = (X^TX + \lambda I)^{-1}X^Ty
$$
因此说， 岭回归本质上是 **线性回归 + L2 正则化**， 从而达到抑制过拟合的效果。

## LWR - 局部加权回归

在线性回归中， 由于最终拟合出来的曲线是一条直线，其拟合能力极为有限（也可以解释为线性回归所求的是具有最小均方误差的无偏估计），因此很容易造成欠拟合现象， 而针对这个问题，有人提出了局部线性回归(LWR)。

局部加权回归其思想很简单： 我们对一个输入 w 进行预测时，赋予了 x 周围点不同的权值，距离 x 越近，权重越高。整个学习过程中误差将会取决于 x 周围的误差，而不是整体的误差，这也就是局部一词的由来。

在LWR中， 其损失函数为：
$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^m w^{(i)} (h_\theta(x^{(i)}) - y^{(i)})^2 \\ 矩阵表示: J(\theta) = \frac{1}{2m} (X\theta-y)^TW(X\theta - y)
$$
此时，使用回归方程求得：

$$
\theta = (X^TWX)^{-1}X^TWy 
$$


而通常， $w^{(i)} $ 服从高斯分布， 在x周围指数型衰减;
$$
w^{(i)} = e^{- \frac{|x^{(i)} - x|}{2 k^2 }} 
$$

其中， **k 值越小，则靠近预测点的权重越大，而远离预测点的权重越小**。所以参数k的值决定了权重的大小。 

> - k越大权重的差距就越小，k越小权重的差距就很大，仅有局部的点参与进回归系数的求取，其他距离较远的权重都趋近于零。
> - 如果k去进入无穷大，所有的权重都趋近于1，W也就近似等于单位矩阵，局部加权线性回归变成标准的无偏差线性回归，会造成欠拟合的现象；
> - 当k很小的时候，距离较远的样本点无法参与回归参数的求取，会造成过拟合的现象。

---

## QA

### 1. 仔细推导一下最小二乘法估计

$$
\begin{align}
L(w) &= \sum_{i=1}^N ||w^Tx_i - y_i ||^2 \\
&= \sum_{i=1}^N (w^Tx_i - y_i)^2 \\
&= \begin{pmatrix} w^Tx_1 - y_1 & ... & w^Tx_N - y_N  \end{pmatrix} \begin{pmatrix} w^Tx_1 - y_1 \\ ... \\ w^Tx_N - y_N  \end{pmatrix}
\end{align}
$$

其中有：
$$
\begin{align}
\begin{pmatrix} w^Tx_1 - y_1 & ... & w^Tx_N - y_N  \end{pmatrix} = \begin{pmatrix} w^Tx_1  & ... & w^Tx_N  \end{pmatrix} - \begin{pmatrix} y_1  & ... & y_N  \end{pmatrix} &=  w^TX^T - Y^T
\end{align}
$$
那么，最终就得到：
$$
\begin{align}
L(w) &=  (w^TX^T - Y^T)(Xw + Y) \\
&= w^TX^TXw - w^TX^TY - Y^TXw - Y^TY
\end{align}
$$
考虑到 $w^TX^TY$ 与 $Y^TXw$ 的结果其实都是一维实数且二者为转置，因此，二者的值相等， 那么就有：
$$
L(w) = w^TX^TXw - 2w^TX^TY - Y^TY
$$
那么就有：
$$
\hat{w} = argmin \, L(w) \\
\frac{\delta L(w)}{\delta w} = 2X^TXw - 2X^TY = 0
$$
从而就得到：
$$
w = (X^TX)^{-1}X^TY
$$

### 2. 从概率角度看最小二乘法

最小二乘法隐藏的一个条件是： 噪声服从正态分布。
$$
\varepsilon \sim N(0, \sigma^2) \\
y = f(w) + \varepsilon = w^Tx + \varepsilon
$$
那么则有：
$$
y |x,w \sim N(w^Tx, \sigma^2); P(y|x;w) = \frac{1}{\sqrt{2\pi\sigma}} e^{-\frac{(y-w^Tx)^2}{2 \sigma^2}} \\
\begin{align}
L(w) &= log P(Y|X;w)  \\
&= log \prod_{i=1}^N P(y_i|x_i; w) \\
&= \sum_{i=1}^N log P(y_i | x_i; w) \\
&= \sum_{i=1}^N (log \frac{1}{\sqrt{2 \pi \sigma}} + log \, e^{-\frac{(y-w^Tx)^2}{2 \sigma^2}}) \\
&= \sum_{i=1}^N (log \frac{1}{\sqrt{2 \pi \sigma}} - -\frac{(y-w^Tx)^2}{2 \sigma^2})
\end{align}
$$
那么，采用极大似然对 w 进行估计有：
$$
\begin{align}
\hat{w} &= argmax \, L(w) \\
&= argmax -\frac{1}{2\sigma^2}(y_i - w^Tx_i)^2 \\
&= argmin(y_i - w^Tx_i)^2
\end{align}
$$




