# 信息论

tags: 基础数学知识

---

## 基本知识

- 基本思想: 一件不太可能的事情发生, 要比一件非常可能的事情发生提供更多的信息

- 性质:

  > - 非常可能发生的事情信息量较少,并且极端情况下,一定能够发生的事件应该没有信息量
  > - 比较不可能发生的事件具有更大的信息量
  > - 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。

## 1. 自信息 ，信息熵，互信息

### 自信息 - self-information

如果说概率P是对确定性的度量，信息是对不确定性的度量，这两者是相对的， **事件发生的概率越大，那么事件的信息量就越小， 事件的概率与事件的信息量之间成反比。**

举例来说：如果时间A发生的概率比事件B发生的概率要大，那么我们就说时间B的信息量要比事件A的信息量要大。

信息量能够量化以上性质,定义一个事件x的自信息为：
$$
I(x) = -log(p(x))
$$
当该对数的底数为自然对数 e 时，单位为奈特（nats）；当以 2 为底数时，单位为比特（bit）或香农（shannons）.

### 信息熵 -- information-entropy

信息熵是对**平均不确定性**的度量，本质上是**所有事件的信息量的期望**。

$$
H(X) = E_{X}[I(x)]=-\sum_{x \in X} p(x)log(p(x)) \\
$$
其中， n表示有n种事件， 每种事件发生的可能性为 $p(x_i)$. 举例来说， 电脑有电脑正常， 电脑死机， 电脑爆炸三种可能事件，那么n就为3。

信息论中，记 `0log0 = 0`

### 互信息

$$
I(X,Y) = \sum_{y \in Y} \sum_{x \in X} p(x,y) log( \frac{p(x,y)}{p(x)p(y)})
$$

互信息 $I(X,Y)$ 取值为非负。当X、Y相互独立时，$I(X,Y)$ 最小为0。

## 2. 相对熵（KL散度） 与 交叉熵

### 1. 相对熵 -- KL 散度 ： Kullback-Leibler divergence

如果对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度来衡量这两个分布的差异。

- 定义： P 对 Q 的KL散度为：

$$
D_P(Q) = E_X [log \, \frac{P(x)}{Q(x)}] = \sum_{x \in X}P(x)log(\frac{P(x)}{Q(x)})
$$

- 含义：在离散型变量的情况下， KL 散度衡量的是：当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号的消息时，所需要的额外信息量。

- 性质：

  > - 非负；KL 散度为 0 当且仅当P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的.
  > - 不对称：$D_p(q) != D_q(p)$

### 2. 交叉熵 - cross entropy

$$
H_P(Q)= -E_{X} log \, Q(x) =-\sum_{x \in X}P(x)log\,Q(x)
$$

### 3. 交叉熵与KL散度的关系

- 针对 Q 最小化交叉熵等价于最小化 P 对 Q 的 KL 散度**，因为 Q 并不参与被省略的那一项。
  $$
  H_P(Q) = H(P) + D_P(Q)最大似然估计中，最小化 KL 散度其实就是在最小化分布之间的交叉熵。
  $$

- 最大似然估计中，最小化 KL 散度其实就是在最小化分布之间的交叉熵。

## 3. 条件熵

