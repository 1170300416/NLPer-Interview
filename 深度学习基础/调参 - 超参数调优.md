# 超参数调优

tags: 深度学习

---

[TOC]

## 1. 超参数一览

### 1. 超参数是什么？

- 参数： 需要训练，指的是模型训练中的权重参数和 bias 参数。
- 超参数： 不需要训练，需要在训练前进行指定，并不断调整。

其实就很多超参数来说，调整的意义并不大，毕竟往往网络的超参数多达几十个，要是都精调的话，那岂不是得 gg， 因此往往是对重要参数精调，对次要参数粗调。

此外，很多 Trick 往往需要一些其他的超参数，对于这部分参数，往往我会遵循原论文，适当的调一调就行。毕竟，Trick 无穷尽呀。

### 2. 网络结构参数

网络参数指的是你自己构建网络结构时的相关参数，如卷积核数量，网络层数等

- CNN 网络参数

  | 超参数 | 说明 | 推荐值 |
  | ------ | ---- | ------ |
  |        |      |        |
  |        |      |        |
  |        |      |        |

- RNN 网络参数

  | 超参数 | 说明 | 推荐值 |
  | ------ | ---- | ------ |
  |        |      |        |
  |        |      |        |
  |        |      |        |

- Transformer 参数

  | 超参数 | 说明 | 推荐值 |
  | ------ | ---- | ------ |
  |        |      |        |
  |        |      |        |
  |        |      |        |

### 2. 优化参数

优化参数指的是反向传播中所涉及到的参数，主要包括：学习率， batch_size， 对应优化器参数， 损失函数参数等。

| 超参数 | 说明 | 推荐值 |
| ------ | ---- | ------ |
|        |      |        |
|        |      |        |
|        |      |        |

### 3. Trick 参数

| 超参数 | 说明 | 推荐值 |
| ------ | ---- | ------ |
|        |      |        |
|        |      |        |
|        |      |        |

## 2. 几个重要的超参数

### 1. 学习率 -- 最重要的超参数

学习率直接控制着梯度更新时的量级，从而直接影响模型的优化与最终的有效容量。 幸运的是，对于学习率的设置，已经有一套行之可效的指导方案了， 针对不同的优化器，有不同的设置区间。 

| 优化器   | 设置范围     |
| -------- | ------------ |
| SGD      | [1e-2 ,1e-1] |
| Momentum | [1e-3, 1e-2] |
| Adagrad  | [1e-3, 1e-2] |
| Adadelta | [1e-2, 1e-1] |
| RMSprop  | [1e-3, 1e-2] |
| Adam     | [1e-3, 1e-2] |
| Nadam    | [1e-3, 1e-2] |

### 2. batch size

一般情况下，  batch size 我往往会以 128 为起点，上下调整，注意，batch size 要设置为 2 的幂次方， 范围在 [1, 1024] 之间。

此外，需要一提的是 Batch Normalization 与 batch size 息息相关，如果你使用了 Batch Normalization， 那么 batch size 就不能设的太小。 

### 3. dropout

dropout 我往往会设置先为 0.5， 然后在 [0.0, 0.5] 范围内精调。

## 超参数调优策略

**采用2的幂次方作为 batch_size 的值，并在对数尺度上对学习率进行采样。**

### 1. 网格搜索

- 定义一个 n 维的网格，每一格都有一个超参数。
- 对于每个维度，定义可能的取值范围
- 搜索所有可能的配置并获得最佳结果

我个人一般还是用 Markdown 表格来做记录， 如下：

| 优化算法 | 学习率 | batch_size |
| -------- | ------ | ---------- |
| adam     | 1e-5   | 128        |

- **缺点：**该方法痛点真的很痛，那就是：**维数灾难**。 随着要精调的超参数的增加，搜索在时间复杂度上也会增加的越多（指数级别），最终使得该策略不可行。
- **优点：** 如果采用较大的搜索范围以及较小步长，该方法有很大概率能找到全局最优值

因此， 我一般尽可能少的去调节次要超参数，比如优化算法默认 Adam 等。此外， 先进行粗调来寻找全局最优值可能的位置，然后采用精调的策略寻找更精确的最优值。

一般只有超参数在 4 个以内才使用网格搜索，不然太费时间了。

### 2. 随机搜索

随机搜索在搜索范围内随机选取样本点，它认为如果样本点集足够大，那么通过随机采样也能大概率的找到全局最优值或其近似值。

- 优点： 比网格搜索要快
- 缺点：结果无法保证，很依靠调参经验。

我一般都是以推荐超参数设置方案来作为第一次的设置，然后围绕这个设置点上下浮动。

### 3. 贝叶斯优化

网格搜索与随机搜索都是独立于之前的训练的，

https://zhuanlan.zhihu.com/p/29779000