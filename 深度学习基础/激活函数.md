# 激活函数

tags: 深度学习

---

[TOC]

## 前言

本文先对激活函数的特性，常见的激活函数以及如何选择合适的激活函数。

**需要注意的是，激活函数是来向神经网络中引入非线性因素的，通过激活函数，神经网络就可以拟合各种曲线。**可参考：[激活函数，你真的懂了吗？](https://zhuanlan.zhihu.com/p/44398148)

## 1. 激活函数的性质



## 2. 激活函数一览











## 激活函数的性质

- 非线性：引入非线性特征
- 可微性：便于反向传播，利于优化
- 单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数；
- $ f(x)≈x $： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；
- 输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。

## 如何选择激活函数

一个很通常的做法是， 将每个合适的激活函数都试试，然后验证集或者测试集上进行评价。

- 如果是二分类问题， 输出层是sigmoid，其余层是Relu
- 一般隐层采用Relu， 有时也要试试 tanh， 这两大函数的变体都有必要试试

## Relu 的优点

- Relu 不耗费资源，且导数为1， 学习起来较快
- sigmoid， tanh 的导数在正负饱和区的梯度都会接近于0， 这会造成梯度消失。
- Relu 有Dead Relu 问题，此时试试其变体

## 激活函数的稀疏激活性

从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $ x<0 $ 时，ReLU 硬饱和，而当 $ x>0 $ 时，则不存在饱和问题。ReLU 能够在 $ x>0 $ 时保持梯度不衰减，从而缓解梯度消失问题。