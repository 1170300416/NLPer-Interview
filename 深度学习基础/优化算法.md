# 优化算法

tags: 深度学习

---

[TOC]

## 简介

优化算法在深度学习中也是十分重要的，虽然一般情况下无脑 Adam 即可，但有时采用其余的优化算法反而能够获得更好的结果，这点很有意思。同时，理解优化算法的原理对于选择和面试还是很有帮助的。

## 三大基本算法

### 1. 随机梯度下降

$$
\theta = \theta + \eta \cdot  \nabla_\theta J(\theta; x^{(i)}; y^{(i)}) \,\,\, \eta 为学习率
$$

### 2. 标准梯度下降

$$
\theta = \theta + \eta \cdot  \nabla_\theta J(\theta)  
$$

### 3. mini-batch 梯度下降

$$
\theta = \theta + \eta \cdot  \nabla_\theta J(\theta; x^{(i:i+n)}; y^{(i:i+n)}) 
$$

### 三者比较

- **随机梯度下降：** 每次更新的方向并不向全局最优解的方向前进，最终的收敛结果也往往在全局最优解附近，但并不能达到最优解。迭代过程不可测。
- **标准梯度下降：** 更新速度很慢，每次都要计算全部样本的梯度。且由于没有随机因素，很容易陷入鞍点或局部最小值。
- **mini-batch 梯度下降：** 二者折中，batch size 的设置是一个艺术。

## 梯度下降算法的一点改进

### 1. 动量梯度下降法

在使用梯度下降算法中，很容易产生一种“震荡现象”（相邻前后梯度正负相反），这种震荡现象减慢了梯度下降法的速度，这也导致你无法使用更大的学习率， 如果你使用较大的学习率， 可能导致震荡更大， 收敛更慢。

**Momentum的核心思想： 通过计算梯度的指数加权平均值，并利用该平均值来更新你的权重，这样梯度的变化就没有那么快了。**
$$
v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta)  \\ \theta = \theta - v_t \\
\gamma: \text{加权系数，常取值为0.9}
$$
举个例子， 假如这里有个碗状的峡谷， 峡谷中坑坑洼洼，有许多小坡， 如果我们从峡谷上推下一个球， 根据物理现象，由于加速度，小球的速度在下降的过程中越来越快， 即使路上遇到小坡，也能够轻松跨越（跨不过去，就是局部最小点了）， 直至到达谷底。

引申到动量法的参数变化中： 对于在梯度点处具有相同的方向的维度，表明加速度为正，那么速度自然加快；对于在梯度点处改变方向的维度， 加速度为负， 速度减缓。 这样，我们可以得到更快的收敛速度， 同时可以减少摇摆。

### 2. Nesterov Accelerated Gradient


$$
v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta- \gamma v_{t-1})  \\ \theta = \theta - v_t
$$


