# 权重初始化方案

tags: 深度学习

---

[TOC]

## 前言

本节先对一些常见的初始化方案进行描述，然后依托于 Pytorch ， 论述 Pytorch 中提供的几个初始化方案，最后提出一些使用建议。

## 权重初始化为何如此重要？

虽然 Batch Normalization， Layer Normalization 等 Trick 大大减轻了我们需要精选权重初始化方案的需要，但对于大多数情况下， 选择合适的初始化方案依旧有利于加速我们模型的收敛。

从根本上看，选择合适的初始化方案能够使得我们的损失函数便于优化（有些优化面坑坑洼洼，有些优化面比较光滑）； 从另一个角度来说， 合适的权重初始化有利于减轻梯度消失，梯度爆炸问题（参考公式推导）。

## 初始化方案

### 1. 常量初始化

- 初始化为 0： 当我们在初始化的时候，最容易想到的就是初始化为0，但是，将W初始化为0， 那么在前向计算的时候，我们的**所有神经元的输出均为相同**， 然后在反向传播中， **梯度相同**， **权重更新相同**，这明显是不可行的。

  所以， **千万不要初始化为0。**

```
torch.nn.init.constant_(tensor, val)
```

###  2. 随机初始化

如果我们采用随机初始化，因为我们不知道我们的参数会初始化为多少， 如果初始化不合理， 造成梯度消失的可能性是相当之大，另一方面，如果初始化在优化面坑坑洼洼的那一面，我们的优化过程将变得异常曲折，局部最小值，鞍点以及大的平坦区会造成优化的噩梦。 

### 2. 均匀分布初始化 - U(a,b)

```
torch.nn.init.uniform_(tensor, a=0.0, b=1.0)
```

### 3. 高斯分布 - N(mean, std)

```
torch.nn.init.normal_(tensor, mean=0.0, std=1.0)
```

### 4.  单位矩阵初始化

```
torch.nn.init.eye_(tensor)
```

### 5. Xavier  初始化

- Xavier 均匀分布

  ```
  torch.nn.init.xavier_uniform_(tensor, gain=1.0)
  ```

- Xavier 正态分布

  ```
  torch.nn.init.xavier_normal_(tensor, gain=1.0)
  ```

### 6. Kaiming 初始化（MSRA初始化）

- Kaiming 均匀分布

  ```
  torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')
  ```

- Kaiming 正态分布

  ```
  torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')
  ```

### 7. 正交初始化

```
torch.nn.init.orthogonal_(tensor, gain=1)
```

### 8. 稀疏初始化

```
torch.nn.init.sparse_(tensor, sparsity, std=0.01)
```



## 预训练

目前 NLP 的趋势已经渐进明确，预训练模型的确能够大大的提高我们的模型效果， 因此，如果对这方面感兴趣，可以看看我相关的博客。 预训练语言模型是如今最火热的一个领域，推荐持续关注，但应该是没有资源训练的。



## Reference

[1]  Xavier Glorot et al., Understanding the Difficult of Training Deep Feedforward Neural Networks

[2]  Kaiming He et al., Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classfication

[聊一聊深度学习的weight initialization](<https://zhuanlan.zhihu.com/p/25110150>)

[吴恩达团队：神经网络如何正确初始化？](<http://www.zhuanzhi.ai/document/b5620e285c10477c1490566d8eeea207>)